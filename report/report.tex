\documentclass[paper=a4, 	% Seitenformat
		fontsize=11pt, 		% Schriftgr\"o\ss{}e
		abstracton, 	% mit Abstrakt
		headsepline, 	% Trennlinie f\"ur die Kopfzeile
		notitlepage	% keine extra Titelseite
		]{scrartcl}
%\usepackage[a5paper,margin=5mm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[english, ngerman]{babel}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{subcaption}
\captionsetup{justification=centering}


\usepackage{mathbbol}
\usepackage{color}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=tikz/]
\usetikzlibrary{matrix} 


\bibliographystyle{alphadin}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\diff}{\,\textrm{d}}
\newcommand{\todo}[1]{{\color{red} #1}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\avg}{\textnormal{avg}}

\newcommand{\transl}[2]{T_{#1}\, #2}
\newcommand{\fNat}[1]{[ #1 ]}


\title{Neuronale Faltungsnetze und Pooling}
\author{Michael Markl}
\date{25. Juni 2021}
\subtitle{Mathematische Aspekte des Maschinellen Lernens\\ Seminar bei Prof. Dr. Stykel}

\begin{document}
\maketitle


\section{Einführung}

Neuronale Netze werden bereits heutzutage auf zahlreiche Problemstellungen angewandt.
Dabei ist Computer Vision -- also die Verarbeitung und Analyse von Bildern -- eine der meist genutzten Technologien, die dadurch ermöglicht wird.
Ein klassisches Problem der Computer Vision ist, ein Bild zu einer bestimmten Kategorie zuzuordnen.
Bei der Erkennung von handschriftlichen Ziffern hat LeCun in~\cite{LeCun1989} aufgezeigt, dass das Reduzieren eines Netzwerks durch das Teilen von Parametern mehrere Neuronen zwischen zwei Layern die topologische 2D-Struktur von Bildern ausnutzen kann und dadurch die Klassifizierung erheblich verbessern kann.

% TODO:
Diese Arbeit gibt einen Einblick in Theorie und Praxis von Faltungsnetzen sowie die oft dabei verwendeten Pooling-Layer.
Als Grundlagen werden die Werke von Ian Goodfellow et al. in~\cite[Kapitel~9]{Goodfellow-et-al-2016} sowie von Ovidiu Calin in~\cite[Kapitel~15,16]{Calin2020} herangezogen.

\section{Faltungslayer}

\subsection{Faltung und Kreuzkorrelation}\label{subsec:convolution}

Bevor wir Faltungsnetze einführen, möchten wir zunächst die zugrundeliegenden mathematischen Konzepte einführen:
Faltung und Kreuzkorrelation.
Diese werden oft in der digitalen Signalverarbeitung genutzt, wo man zwischen kontinuierlichen und diskreten Signalen unterscheidet.

\newcommand{\llambda}{\lambda}
\newcommand{\cmeasure}{\mu_c}

\begin{definition}[Signale]
    Ein \emph{kontinuierliches Signal} ist eine Funktion $f: \R^d\rightarrow \R$.
    Im Kontext von kontinuierlichen Signalen wird das Lebesgue-Maß $\llambda$ auf $\Omega\coloneqq \R^d$ verwendet.

    Ein \emph{diskretes Signal} ist eine Funktion $f: \Z^d \rightarrow \R$.
    Bei diskreten Signalen wird das Zählmaß $\cmeasure$ auf $\Omega\coloneqq \Z^d$ verwendet.

    Ein Signal $f: \Omega \rightarrow \R$
    \begin{itemize}
        \item \emph{hat einen kompakten Träger}, falls es ein $R > 0$ gibt, sodass $f(x) = 0$ für $\norm{x}_\infty \geq R$ gilt,
        \item heißt \emph{$L^1$-endlich}, falls $\norm{f}_1 \coloneqq \int_{\Omega} \abs{f(x)} \diff \mu(x) < \infty$,
        \item heißt \emph{Energie-Signal}, falls $\norm{f}_2 \coloneqq \left(\int_{\Omega} f(x)^2 \diff \mu(x) \right)^{1/2} < \infty$.
    \end{itemize}
\end{definition}

\begin{proposition}
    Ein kontinuierliches Energie-Signal mit kompaktem Träger ist $L^1$-endlich.
    Außerdem sind diskrete Signale mit kompaktem Träger $L^1$-endlich.
\end{proposition}
\begin{proof}
    Sei $f$ ein kontinuierliches Energie-Signal mit $f(x) = 0$ für $\norm{x}_{\infty} \geq R$.
    Sei $\mathbb{1}_R$ die charakteristische Funktion mit $\mathbb{1}_R(x) = 1$ für $\norm{x}_\infty \leq R$ und $\mathbb{1}_R(x) = 0$ sonst.
    Sei außerdem $\langle \,\cdot\, , \,\cdot\, \rangle_{L_2}$ das Skalarprodukt auf dem Hilbertraum $L^2$.
    Dann gilt mit der Cauchy-Schwarz-Ungleichung
    \[
        \norm{f}_1 
        = \int_{\R^d} \abs{f(x)} \cdot \mathbb{1}_R(x) \diff x
        = \langle \abs{f}, \mathbb{1}_R \rangle_{L^2}
        \leq \norm{\abs{f}}_2 \cdot \norm{\mathbb{1}_R}_2 < \infty .
    \]

    Für diskrete Signale mit kompaktem Träger folgt die $L^1$-Endlichkeit bereits aus der Endlichkeit der Menge $\{ z\in\Z^d \mid \norm{z}_{\infty} \leq R\}$.
\end{proof}

\begin{definition}
    Die \emph{Faltung} zweier diskreter bzw. kontinuierlicher Signale $f,g: \Omega\rightarrow \R$ ist definiert als 
    \[
        (f * g)(x) \coloneqq \int_\Omega f(\tau) \cdot g(x-\tau) \diff \mu(\tau).
    \]
    Die \emph{Kreuzkorrelation} von $f$ und $g$ ist definiert als
    \[
        (f \star g)(x) \coloneqq \int_\Omega f(\tau) \cdot g(x+\tau) \diff \mu(\tau).
    \]
    Dabei wird $f$ meist der \emph{Kern} oder \emph{Filter} und $g$ das \emph{(Eingangs-)Signal} genannt.
\end{definition}

Wir bemerken, dass der einzige Unterschied der Operationen darin besteht, dass bei der Faltung der Kern \glqq geflippt\grqq\ wird.
Es gilt nämlich \[
    \left(f \star g\right)(x) = \int_{\Omega} f(\tau) \cdot g(x+\tau) \diff\mu(\tau) = \int_\Omega f(-\tau) \cdot g(x - \tau) \diff\mu(\tau) = \left((\tau\mapsto f(-\tau)) * g\right)(x).
\]
Ist der Kern \emph{symmetrisch}, gilt also $f(x) = f(-x)$ für alle $x\in\Omega$, so stimmt Kreuzkorrelation mit Faltung überein.
Durch das \glqq Flippen\grqq\ des Kerns wird die Faltungsoperation kommutativ, was für die Kreuzkorrelation nicht gilt.
Oft wird die Faltung bzw. Kreuzkorrelation so dargestellt, dass der Kern diejenige Funktion ist, welche verschoben und gegebenenfalls geflippt wird:
\[
    (f*g)(x) = \int_\Omega f(x - \tau) \cdot g(\tau)\diff\mu(\tau), \quad
    (f\star g)(x) = \int_\Omega f(\tau - x) \cdot g(\tau) \diff\mu(\tau).
\]

\begin{proposition}
    Sind $f$ und $g$ $L^1$-endliche Signale, so auch $f * g$ und $f\star g$.
\end{proposition}
\begin{proof}
    Seien $f$ und $g$ $L^1$-endliche Signale. Wir nutzen den Satz von Fubini:
    \begin{align*}
        \norm{f * g}_1
        &= \int_{\Omega} \abs{ \int_\Omega f(\tau)\cdot g(x - \tau) \diff\mu(\tau) }\diff \mu(x)
        \leq \int_{\Omega}  \int_{\Omega} \abs{f(\tau)\cdot g(x - \tau)} \diff\mu(\tau) \diff \mu(x)\\
        &= \int_{\Omega} \abs{f(\tau)} \cdot \int_{\Omega} \abs{g(x-\tau)} \diff \mu(x) \diff\mu(\tau)
        = \norm{f}_1 \cdot \norm{g}_1 < \infty.
    \end{align*}
    Für $f \star g$ funktionieren die Schritte analog.
\end{proof}

Im Deep-Learning-Kontext wird meist von Faltung gesprochen, dabei wird aber tatsächlich oft die Kreuzkorrelation gemeint und verwendet.
\todo{Später werden wir sehen, warum dies keine größeren Auswirkungen auf die Neuronalen Faltungslayer hat.}
Wir werden daher im Folgenden meist nur die Kreuzkorrelation näher betrachten.

Wir betrachten zunächst einige Beispiele dieser Operationen.
Definiere dazu die Signale
\begin{align*}
    f: \R \rightarrow \R, \quad x \mapsto \begin{cases}
        \frac{1}{2} - x, & \text{für $x\in[-\frac{1}{2},\frac{1}{2}]$,}\\
        0, & \text{sonst,}
    \end{cases} \qquad
    g: \R \rightarrow \R,\quad x \mapsto \begin{cases}
        1,& \text{für $x \in [-\frac{1}{2},\frac{1}{2}]$}\\
        0, & \text{sonst.}
    \end{cases}
\end{align*}

In Abbildung~\todo{figure-a} kann man die Faltung $f * g$ der beiden Signale sehen, in Abbildung~\todo{figure-b} die Kreuzkorrelation~$f\star g$.


In Anwendungen werden meist diskrete Signale mit kompaktem Träger betrachtet.
Ein diskretes Signal dieser Form schreiben wir im eindimensionalen Fall $d=1$ als Vektor $f = (f_0, f_1, \dots, f_{n-1})$, wobei wir für eine leichtere Notation stets bei $0$ anfangen zu indizieren.
Dazu führen wir als Notation $\fNat{n}\coloneqq \{0, \dots, n-1\}$ ein. 
Zweidimensionale Signale mit kompaktem Träger schreiben wir demnach als Matrix $$F = (f_{i,j})_{\substack{i\in\fNat{m}\\ j\in\fNat{n}}}.$$
Diese Signale werden für Indizes außerhalb der so-definierten Vektoren bzw. Matrizen als $0$ ausgewertet, also beispielsweise $f_z = 0$ für $z\in\Z\setminus\fNat{n}$ mit $f=(f_0, \dots, f_{n-1})$.

Betrachtet man die Kreuzkorrelation von $g$ mit dem Filter $f=(\frac{1}{2}, \frac{1}{2})$, so erhält man den sogenannten \emph{gleitenden Mittelwert} von $g$:
\[  
    (f \star g)_i = \int_\Z f_\tau \cdot g_{i + \tau} \diff \cmeasure(\tau) 
    = \sum_{\tau\in\Z} f_\tau \cdot g_{i+ \tau}
    = \frac{g_i}{2} + \frac{g_{i+1}}{2} = \frac{g_i + g_{i+1}}{2}
\]

Um Kreuzkorrelation auf Graustufen-Bilder auszuführen, bestimmt man zu\-nächst eine Input-Enkodierung.
Dabei wird oft eine Matrix $G\in[0,1]^{r\times c}$ gewählt, wobei $r$ die Anzahl an Reihen, $c$ die Anzahl an Spalten und $g_{i,j}$ die \emph{Aktivierung des Pixels} zwischen $0$ und $1$ sind, wobei der Wert~$0$ für Schwarz und der Wert~$1$ für Weiß stehen.
Der sogenannte \emph{Sobel-Filter} zur Erkennung von horizontalen Kanten hat die Form
\[
    F = \begin{pmatrix}
        1 & 2 & 1 \\
        0 & 0 & 0 \\ 
        -1 & -2 & -1
    \end{pmatrix}.
\]


\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \resizebox{\textwidth}{\textwidth}{
        \input{2d-conv-layer.tex}
        }
        \caption{Das Input-Signal $G$.\\Schwarz $\hat=$ $0$, Weiß $\hat=$ $1$.}
    \end{subfigure}%
    \begin{subfigure}{0.4\textwidth}
        \resizebox{0.9285\textwidth}{0.9285\textwidth}{
        \input{2d-conv-layer-convolved.tex}
        }
    \caption{Die Kreuzkorrelation $F\star G$.\\Schwarz $\hat=$ $-3{,}84$, Weiß $\hat=$ $3{,}98$.}
    \end{subfigure}
    \caption{Die Kreuzkorrelation ohne Padding einer handgeschriebenen Ziffer mit dem Sobel-Filter $F$ zur Erkennung von horizontalen Kanten.}
    \label{fig:sobel-on-mnist}
\end{figure}

Bei der Berechnung der Kreuzkorrelation $F\star G$ mit einem Eingangssignal~$G$ hilft die Vorstellung, dass der Kern $F$ schrittweise über den Input geschoben wird und in jedem Schritt ein Output-Pixel als die durch den Kern gewichtete Summe der Eingangspixel berechnet wird.
Dabei erhält man für den Sobel-Filter~$F$:
\[
(F\star G)_{i,j} = \begin{matrix*}[l]
    \hphantom{+}1\cdot G_{i,j}  & + 2\cdot G_{i, j+1} & + 1\cdot G_{i, j+2} \\
    + 0\cdot G_{i+1, j} &+ 0\cdot G_{i+1, j+1} &+ 0\cdot G_{i+1, j+2} \\
    +1\cdot G_{i+1, j} &+ 2\cdot G_{i+1, j+1} &+ 1\cdot G_{i+2, j+2}.
    \end{matrix*}
\]

In Abbildung~\ref{fig:sobel-on-mnist} sieht man die Kreuzkorrelation des Sobel-Filters am Beispiel einer handschriftlichen Ziffer aus der MNIST-Datenbank aus~\cite{lecun2010mnist}.
Das Ausgangsbild $G$ hat $28\times 28$ Pixel.
Wir betrachten zumeist nur solche Pixel von $F\star G$, in denen der gesamte Filter im \glqq relevanten Teil\grqq\ des Inputs, also innerhalb dieser $28\times 28$ Pixel ist.
Daher hat das Output-Bild in beiden Längen $2$ Pixel weniger als $G$.
Diese Technik nennt man Kreuzkorrelation \emph{ohne Padding}.
Allgemeiner: Sei $G\in\R^{n_1\times\cdots \times n_d}$ ein Eingangssignal und $F\in\R^{m_1\times\cdots\times m_d}$ ein Filter mit $n_i \geq m_i$, so hat die Kreuzkorrelation von $F$ und $G$ ohne Padding die Form $H\in\R^{k_1\times\cdots\times k_d}$ mit $k_i = n_i - m_i + 1$. 
\todo{In Abschnitt bla werden weitere Varianten der Kreuzkorrelation beleuchtet, die bei Faltungslayern zum Einsatz kommen.}




\subsection{Eindimensionale Faltungslayer}

Wir betrachten zunächst eindimensionale Faltungslayer; das heißt, in diesem Abschnitt werden nur eindimensionale Eingangssignale beachtet.
Ein Beispiel für ein solches Eingangssignal ist ein Audiosignal, welches in einer bestimmten Frequenz abgetastet wird.

In einem Faltungslayer will man bestimmen, wie sehr das Eingangssignal mit einem bestimmten, gelernten Kern korreliert.
Zusätzlich zu diesem Kern wird außerdem ein Bias gelernt, der zur Kreuzkorrelation addiert wird.
Schließlich wird der Output des Layers durch eine Aktivierungsfunktion ausgewertet an dieser Summe berechnet.

Seien also $x=(x_0,\dots,x_{n-1})\in\R^n$ ein Eingangssignal und $w=(w_0,\dots,w_{m-1})\in\R^m$ der gelernte Kern, $b\in\R$ der Bias und $\sigma: \R\rightarrow\R$ die Aktivierungsfunktion.
Dann lässt sich der Output $y$ des Layers beschreiben als
\[
    y_i = \sigma\left( (w \star x)_i + b  \right)
    = \sigma\left( \sum_j w_j \cdot x_{i+j} + b \right).
\]
Wie in Abschnitt~\ref{subsec:convolution} beschrieben, werden wir hier Kreuzkorrelation ohne Padding verwenden, sodass $y$ genau ${n-m+1}$ Einträge hat.


\begin{figure}
\begin{subfigure}{0.5\textwidth}
    \centering
    \input{1d-conv-layer.tex}
    \caption{Ein Eindimensionales Faltungslayer mit $n=5,m=2$.
    Viele Verbindungen teilen sich die drei Parameter $w_0$, $w_1$ und $b$.
    }
    \label{fig:one-dimensional-conv-layer}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
    \centering
    \input{1d-dense-layer.tex}
    \caption{Ein Dense-Layer. Jede Verbindung hat ein eigenes Gewicht und jedes Output-Neuron einen eigenen Bias.}
    \label{fig:one-dimensional-dense-layer}
\end{subfigure}
    \caption{Zwei eindimensionale Layer mit 6 Input- und 5 Output-Neuronen.}
    \label{fig:one-dimensional-layers}
\end{figure}

In Abbildung~\ref{fig:one-dimensional-conv-layer} kann man ein Beispiel eines solchen Faltungslayers sehen.
Im Vergleich zu einem Dense-Layer in Abbildung~\ref{fig:one-dimensional-dense-layer} hat das Faltungslayer hier nur $10$ statt $30$ Verbindungen und nur $3$ statt $35$ lernbare Parameter.
Trotzdem kann ein einzelnes Faltungslayer mit vergleichsweise kleinem Kern vor allem lokale Eigenschaften und Muster bereits sehr gut erkennen.

Da die Kreuzkorrelation prüft, wie sehr sich ein Eingangssignal dem Kern an einer bestimmten Stelle ähnelt, nennen wir den Output $y$ auch oft eine \emph{Feature-Map}.
Wir können $y$ nämlich auffassen als Karte, auf der man nachsehen kann, wie stark ein Merkmal an einer bestimmten Stelle im Eingangssignal ausgeprägt ist.

Dabei ist es oftmals interessant, in einem Layer parallel mehrere Feature-Maps zu berechnen, da auch andere Merkmale aus dem Eingangssignal für die Minimierung der Zielfunktion interessant sein können.
In größeren neuronalen Netzen wird die Verwendung von Faltungslayern aus diesem Grund oft als \emph{Feature-Extraktion} bezeichnet.
Umgesetzt wird dies, indem mehrere Kernel gleichzeitig auf denselben Eingangsdaten operieren und so mehrere Feature-Maps erzeugen.

Weil man häufig Faltungslayer hintereinander schaltet, müssen diese als Input auch mehrere solcher Feature-Maps akzeptieren.
Ein weiterer Grund mehrerer Eingangskanäle kann sein, dass die Input-Daten des Netzes bereits mehrere Feature-Maps beinhalten:
Beispielsweise kann eine Audiodatei mehrere Tonspuren enthalten, in der jede der Spuren ein eigenes eindimensionales Signal ist, in der aber alle Spuren zeitlich aufeinander abgestimmt sind.
Daher hat jeder Kern meistens eine weitere Dimension, deren Größe der Anzahl der eingehenden Features entspricht, sodass alle Input-Feature-Maps in die Berechnung einer jeden neuen Output-Feature-Map einbezogen wird.

Die vorangegangene Diskussion mündet in der folgenden Definition von eindimensionalen Faltungslayern, in der das Konzept mehrerer Feature-Maps mathematisch eingebettet wird.

\begin{definition}[Eindimensionales Faltungslayer]\label{def:one-dimensional-conv-layer}
    Ein eindimensionales Faltungslayer, welches das $l$-te Layer eines Netzes ist, hat die folgenden Eigenschaften:
    \begin{itemize}
        \item die räumliche Input-Größe $n^{(l-1)}$ und die Anzahl an Input-Feature-Maps $f^{(l-1)}$,
        \item die räumliche Kernel-Größe $m^{(l)}\leq n^{(l-1)}$ und die Anzahl an Output-Feature-Maps $f^{(l)}$ und
        \item die Aktivierungsfunktion $\sigma^{(l)}$.
    \end{itemize}
    Inputs des Layers haben die Form $x^{(l-1)}\in\R^{n^{(l-1)} \times f^{(l-1)}}$.
    Es werden $f^{(l)}$ Kerne und Biases der Form $w^{(l),k}\in\R^{m^{(l)} \times f^{(l-1)}}, b^{(l),k}\in\R$ für  $k\in \fNat{f^{(l)}}$ gelernt.
    Die räumliche Output-Größe ist $n^{(l)} = n^{(l-1)}-  m^{(l)} + 1$.
    Outputs des Layers sind von der Form $x^{(l)}\in \R^{n^{(l)}\times f^{(l)}}$ und für $i\in\fNat{n^{(l)}}$ und $k\in\fNat{f^{(l)}}$ gilt
    \begin{align*}
        x^{(l)}_{i,k}
        &=  \sigma^{(l)}\left( (w^{(l),k} \star x^{(l-1)})_{i,0} + b^{(l),k} \right)\\
        &= \sigma^{(l)} \left( 
            \sum_{j \in \fNat{n^{(l-1)}}} \sum_{k'\in \fNat{f^{(l-1)}}}
            w^{(l),k}_{j,k'} \cdot x^{(l)}_{i+j, k'} + b^{(l), k}
        \right).
    \end{align*}
\end{definition}

\todo{Maybe add another figure with multiple input and output feature maps}

\subsection{Mehrdimensionale Faltungslayer}

Die Definition~\ref{def:one-dimensional-conv-layer} von eindimensionalen Faltungslayern lässt sich leicht auf mehrere Dimensionen erweitern.
Für $d$-dimensionale Eingangssignale $x\in\R^{n_1\times \cdots n_d}$ mit nur einer Feature-Map als Input benötigt man ebenfalls einen $d$-dimensionalen Kern der Form $w\in\R^{m_1\times\cdots m_d}$, sodass sich die folgende Output-Feature-Map ergibt:
\[
    y_{i_1,\dots,i_d} = \sigma\left((w\star x)_{i_1,\dots,i_d} + b\right)
    = \sigma\left( \sum_{j_1,\dots,j_d} w_{j_1,\dots,j_d} \cdot x_{i_1+j_1,\dots,i_d+j_d} + b \right).
\]
Wird wie zuvor Kreuzkorrelation ohne Padding verwendet, hat die Feature-Map $y$ genau $\prod_{i=1}^d (n_i - m_i +1)$ Einträge.
Fässt man die Indizes $i_1,\dots,i_d$ und $j_1,\dots,j_d$ zu je einem Vektor $i$ und $j$ zusammen, erhält man mit $J\coloneqq \bigtimes_{i=1}^d [m_i]$ die etwas vereinfachte Schreibweise
\[
    y_{i} = \sigma\left((w\star x)_{i} + b\right)
    = \sigma\left( \sum_{j \in J} w_j \cdot x_{i+j} + b \right) \quad \text{für $i\in I\coloneqq \bigtimes_{j=1}^d [n_j - m_j +1]$}.
\]
Das führt zur verallgemeinerten Definition, die sich nur bei den räumlichen Dimensionen von der eindimensionalen unterscheidet:

\begin{definition}[Mehrdimensionales Faltungslayer]\label{def:multi-dimensional-conv-layer}
    Ein $d$-dimensionales Faltungslayer, welches das $l$-te Layer eines Netzes ist, hat die folgenden Eigenschaften:
    \begin{itemize}
        \item die räumliche Input-Größe $n^{(l-1)}_1\times\cdots\times n^{(l-1)}_d$ und die Anzahl an Input-Feature-Maps $f^{(l-1)}$,
        \item die räumliche Kernel-Größe $m^{(l)}_1\times \cdots \times m^{(l)}_d$ mit $m^{(l)}_i \leq n^{(l-1)}_i$ und die Anzahl an Output-Feature-Maps $f^{(l)}$ und
        \item die Aktivierungsfunktion $\sigma^{(l)}$.
    \end{itemize}
    Inputs $x^{(l-1)}$ des Layers haben $n^{(l-1)}_1\times\cdots\times n^{(l-1)}_d \times f^{(l-1)}$ Einträge.
    Es werden $f^{(l)}$ Kerne der Form $w^{(l),k}$ mit je $m^{(l)}_1\times\cdots\times m^{(l)}_d \times f^{(l-1)}$ Einträgen und Biases $b^{(l),k}\in\R$ für  $k\in \fNat{f^{(l)}}$ gelernt.
    Die räumliche Output-Größe ist $n^{(l)}_i = n^{(l-1)}_i-  m^{(l)}_i + 1$ für $i\in\{1,\dots, d\}$.
    Wir schreiben $J^{(l)} \coloneqq \bigtimes_{i=1}^d [m^{(l)}_i]$ und $I^{(l)} \coloneqq \bigtimes_{i=1}^d [n^{(l)}_i]$.
    Outputs des Layers sind von der Form $x^{(l)}\in \R^{n^{(l)}\times f^{(l)}}$ und für $i\in I^{(l)}$ und $k\in\fNat{f^{(l)}}$ gilt
    \begin{align*}
        x^{(l)}_{i,k}
        &=  \sigma^{(l)}\left( (w^{(l),k} \star x^{(l-1)})_{i,0} + b^{(l),k} \right)\\
        &= \sigma^{(l)} \left( 
            \sum_{j \in J^{(l)}} \sum_{k'\in \fNat{f^{(l-1)}}}
            w^{(l),k}_{j,k'} \cdot x^{(l)}_{i+j, k'} + b^{(l), k}
        \right).
    \end{align*}
\end{definition}

Die in der Praxis am häufigsten verwendeten Faltungslayer sind solche von Dimension~$2$, da diese im Bereich der Computer-Vision von besonderer Relevanz sind.
Hier hat man meist Bilder als Eingangsdaten.
Ein farbiges Bild wird beispielsweise meist mithilfe von drei Kanälen für die Farben Rot, Grün und Blau -- also in drei zweidimensionalen Feature-Maps -- dargestellt.
Beispiele von dreidimensionalen Signalen sind etwa 3D-Objekte oder Zeitreihen von zweidimensionalen Signalen wie zum Beispiel Filme.

\subsection{Die Rolle der Aktivierungsfunktion}

In diesem Abschnitt möchten wir die Relevanz der Nichtlinearität der Aktivierungsfunktion von Faltungslayern beleuchten.
Dazu betrachten wir wieder eindimensionale Faltungslayer und die Funktionsvorschrift $y_i = \sigma( (w\star x)_i + b )$ mit $x\in\R^n, w\in\R^m$.

Wie Dense-Layer lässt sich auch die Vorschrift von Faltungslayern in eine Matrixform bringen.
Sei dazu $k\coloneqq n-m+1$ und $B\coloneqq (b, \dots, b)\in\R^k$ der Vektor, der in allen Einträgen den Bias $b$ enthält.
Für einen Vektor $v\in\R^k$ sei $\sigma(v) \coloneqq (\sigma(v_0), \cdots, \sigma(v_{k-1}))$.
Wir konstruieren eine Matrix $W\in\R^{k\times n}$ mit der Eigenschaft $y = \sigma( W\cdot x + B )$.
Der $i$-te Eintrag von $W\cdot x$ muss also mit $\sum_{j\in\fNat{m}} w_j \cdot x_{i+j}$
übereinstimmen.
Dadurch hat $W$ die Form einer sogenannten \emph{Toeplitz-Matrix}, d.\,h. einer Matrix, in der ein Eintrag $W_{i,j}$ nur von der Diagonale $i-j$ abhängt.
Denn, der Eintrag $W_{i,j}$ ist $0$ für $j < i$, $w_{j-i}$ für $i \leq j < i+m$ und wieder $0$ für $j \geq i+m$.
Die so durch $w$ erzeugte Toeplitz-Matrix hat die Form
\[
    W = \begin{pmatrix}
            w_0 & \cdots & w_{m-1} & 0 & \cdots & 0 \\
            0 & \ddots & & \ddots  & & \vdots \\
            \vdots  & & \ddots & & \ddots & 0 \\[4pt]
             0 & \cdots & 0 & w_0 & \cdots & w_{m-1}
        \end{pmatrix}.
\]

Im Gegensatz zu Dense-Layern ist hier jedoch nicht jeder Eintrag veränderbar bzw. lernbar.
Stattdessen sind die meisten Einträge festgelegt durch $0$-en, andere Einträge teilen sich einen gemeinsamen Parameter des Kerns.

Wir betrachten nun Faltungslayer, in denen als Aktivierungsfunktion nur die lineare Aktivierungsfunktion $\sigma(x) \coloneqq x$ verwendet wird.
Das nächste Theorem zeigt, dass man in diesem Fall mehrere in Reihe geschalteten Faltungslayer mit einem einzigen Faltungslayer ersetzen kann.
Dabei beschränken wir uns zunächst auf Faltungslayer mit je nur einer Input- und Output-Feature-Map.

\begin{theorem}[Komposition von eindimensionalen Faltungslayern]\label{thm:matrix-collapse-linear-conv-layers}
    Seien $w^{(1)}\in\R^{m^{(1)}}$, $w^{(2)}\in\R^{m^{(2)}}, \dots, w^{(L)}\in\R^{m^{(L)}}$ die (einzigen) Kernel von in Reihe geschalteten eindimensionalen Faltungslayern mit Biases $b^{(1)},\dots, b^{(L)}\in\R$ und linearer Aktivierungsfunktionen $\sigma^{(l)}(x) \coloneqq x$ für $l\in\{1,\dots,L\}$.

    Dann gibt es ein äquivalentes Netz mit nur einem Faltungslayer mit linearer Aktivierungsfunktion und Kernel $w\in\R^m$ mit $m=\sum_{i=1}^L m_i - (L-1)$.
\end{theorem}
\begin{proof}
    Es genügt, die Aussage für $L=2$ zu zeigen, da der Rest per Induktion über $L$ folgt:

    Der Induktionsanfang bei $L=1$ gilt trivialerweise.
    Die Aussage sei also für $L$ erfüllt und wir wünschen sie für $L+1$ zu zeigen.
        
    Nach Induktionsvoraussetzung können die ersten $L$ Layer zu einem äquivalenten Faltungslayer mit Kernel $w'\in \R^{m'}$ mit $m'=\sum_{i=1}^L m_i - (L-1)$ überführt werden.
    Gilt die Aussage für $L=2$, so können wir dieses Layer mit dem $(L+1)$-ten Layer fusionieren, was ein äquivalentes Faltungslayer mit einem Kernel $w\in\R^m$ ergibt mit $m = m' + m^{(L+1)} - (2-1)$.
    Dies lässt sich zu $m = \sum_{i=1}^{L+1} m^{(i)} - (L+1 - 1) $ vereinfachen, was die Indutkion beschließt.
    
    Wir zeigen also den Fall $L=2$. Es seien $\alpha\in \R^k$ und $\beta\in\R^l$ die Kerne zweier in Reihe geschalteten Faltungslayer mit zugehörigen Biases $b_1$ und $b_2$.
    Die von $\alpha$ und $\beta$ erzeugten Toeplitz-Matrizen haben die Form
    \setlength\arraycolsep{2pt}
    \def\arraystretch{0.5}
    \[
        \mathcal{A} = \left(
            \begin{matrix}
                \alpha_0 & \cdots & \alpha_{k-1} & 0 & \cdots & 0 \\
                0 & \ddots & & \ddots  & & \vdots \\
                \vdots  & & \ddots & & \ddots & 0 \\[4pt]
                    0 & \cdots & 0 & \alpha_0 & \cdots & \alpha_{k-1}
            \end{matrix}
        \right),\quad
        \mathcal{B} = \left(
            \begin{matrix}
                \beta_0 & \cdots & \beta_{l-1} & 0 & \cdots & 0 \\
                0 & \ddots & & \ddots  & & \vdots \\
                \vdots  & & \ddots & & \ddots & 0 \\[4pt]
                    0 & \cdots & 0 & \beta_0 & \cdots & \beta_{l-1}
            \end{matrix}
        \right),
    \]
    mit $\mathcal{A}\in\R^{n - k + 1\times n}$ und $\mathcal{B}\in \R^{n-k-l+2 \times n-k+1}$.
    Sind $B_1\coloneqq (b_1,\dots,b_1)$ und $B_2\coloneqq (b_2,\dots,b_2)$ die Bias-Vektoren wie in der Matrix-Form beschrieben, lässt sich der Output des gemeinsamen Netzwerks ausdrücken als
    \[
        \mathcal{B}\cdot \left( \mathcal{A}\cdot x + B_1 \right) + B_2
        = \mathcal{B}\cdot \mathcal{A}\cdot x + \mathcal{B}\cdot B_1 + B_2.
    \]
    Jeder Eintrag des Vektors $(\mathcal{B}\cdot B_1 + B_2)$ stimmt mit der Summe $\sum_{j\in\fNat{l}} \beta_j\cdot b_1 + b_2 \eqqcolon b$ überein, sodass $b$ eine geeignete Wahl als Bias unseres neuen Faltungslayers darstellt.
    
    Es bleibt zu zeigen, dass $\mathcal{B}\cdot \mathcal{A}$ die von einem Kernel der Länge $m\coloneqq k+l-1$ erzeugte Toeplitz-Matrix ist. 
    Für die $i$-te Zeile dieser Matrix gilt
    \[
        (\mathcal{B} \cdot \mathcal{A})_i
        = \begin{pmatrix}
                \smash[b]{\underbrace{\begin{matrix}0 & \cdots & 0\end{matrix}}_{i-1}} & \beta_0 & \beta_1 & \cdots & \beta_{l-1} & 0 & \cdots & 0
            \end{pmatrix} \cdot \mathcal{A}, \\[1em]
    \]
    sodass wir lediglich die $l$ Zeilen ab der $i$-ten Zeile von $\mathcal{A}$ betrachten können:
    \[
        (\mathcal{B} \cdot \mathcal{A})_i = 
            \begin{pmatrix}
                \beta_0 & \beta_1 & \cdots & \beta_{l-1}
            \end{pmatrix} \cdot \begin{pmatrix}
                \mathbb{0}_{l\times (i-1)} & \smash[b]{\underbrace{\begin{matrix}
                    \alpha_0 & \cdots & \alpha_{k-1} & 0 & \cdots & 0 \\
                    0 & \ddots & & \ddots  & & \vdots \\
                    \vdots  & & \ddots & & \ddots & 0 \\[4pt]
                    0 & \cdots & 0 & \alpha_0 & \cdots & \alpha_{k-1} \\
            \end{matrix}}_{l + k - 1 = m}} & \mathbb{0}_{l\times (\geq 0)}\\[2em]
        \end{pmatrix}. \\[2em]
    \]
    Ausgehend von diesem Ausdruck lässt sich ein Eintrag von $\mathcal{B}\cdot \mathcal{A}$ berechnen als
    \[
        (\mathcal{B}\cdot \mathcal{A})_{i,j} = \begin{cases}
            0, & \text{für $j < i$,}\\
            \sum_{p\in \fNat{l}} \beta_{p} \cdot \alpha_{j - i - p}, & \text{für $i \leq j < i+m$,}\\
            0, & \text{für $j \geq i+m$,}
        \end{cases}
    \]
    wobei wir wie in Abschnitt~\ref{subsec:convolution} beschrieben die Konvention $\alpha_p = 0$ für $p\notin \fNat{k}$ verwenden.
    Betrachtet man diesen Ausdruck etwas genauer, so fällt auf, dass die Einträge nur von $i-j$ abhängen und $\mathcal{B}\cdot \mathcal{A}$ somit per Definition eine Toeplitz-Matrix ist.
    Diese Toeplitz-Matrix wird von dem neuen Kern $w_j \coloneqq \sum_{p\in\fNat{l}} \beta_p \cdot \alpha_{j - p}$ für $j\in\fNat{m}$ erzeugt, wodurch die Behauptung folgt.
\end{proof}

Dieses Theorem lässt für den eindimensionalen Fall den unmittelbaren Schluss zu, dass es wenig sinnvoll ist mehrere Faltungslayer mit je nur einer Feature-Map mittels linearen Aktivierungsfunktionen in Reihe zu schalten.
Die Beweisidee stammt dabei von Ovidiu aus~\cite{Calin2020}, wo anhand von Beispielen vorgeschlagen wird, die Matrixform auszunutzen.
Wir betrachten erneut den Kern, der beim Fusionieren zweier Faltungslayer ensteht.
Dieser hat die Form $w_j \coloneqq \sum_{p\in\fNat{l}} \beta_p \cdot \alpha_{j-p}$ für $j\in\fNat{m}$.
Dies entspricht der Faltung von $\beta$ und $\alpha$, also $w_j = (\beta * \alpha)_j$ für $j\in\fNat{m}$.
Allgemeiner können wir die folgende Beziehung von Kreuzkorrelation und Faltung zeigen:
\begin{proposition}\label{prop:associativity-cross-corr}
    Für $L^1$-endliche Signale $f,g,h$ gilt $f\star (g \star h) = (f*g)\star h$.
\end{proposition}
\begin{proof}
    Wir setzen zunächst die Definition von Kreuzkorrelation ein und substituieren anschließend $\tau' \mapsto \tau' - \tau$:
    \begin{align*}
        (f\star (g\star h))(x)
        &= \int_\Omega f(\tau) \cdot \int_\Omega g(\tau') \cdot h(x+\tau+\tau')\diff\mu(\tau') \diff \mu(\tau) \\
        &= \int_\Omega \int_\Omega f(\tau) \cdot g(\tau' - \tau)\cdot h(x+\tau')\diff\mu(\tau') \diff \mu(\tau).
    \end{align*}
    Nun können wir mit dem Satz von Fubini die Integrale tauschen und erhalten
    \begin{align*}
        (f\star (g\star h))(x)&= \int_\Omega\int_\Omega f(\tau) \cdot g(\tau'-\tau) \diff\mu(\tau) \cdot h(x+\tau')\diff\mu(\tau') \\
        &= \int_\Omega  (f * g)(\tau')\cdot h(x+\tau') \diff \mu(\tau')
        = \left( (f * g) \star h \right)(x).\qedhere
    \end{align*}
\end{proof}

Damit lässt sich das Resultat des obigen Theorems leicht für mehrdimensionale Layer verallgemeinern.
Jedoch wollen wir die Aussage nun sogar für mittels linearer Aktivierungsfunktion in Reihe geschalteten Faltungslayern mit beliebiger Anzahl an Feature-Maps beweisen.

\begin{theorem}
    Sei ein Netz mit $L$ in Reihe geschalteten $d$-dimensionalen Faltungslayern gegeben, deren Aktivierungsfunktionen die lineare Aktivierungsfunktion $\sigma(x) \coloneqq x$ sind.

    Dann gibt es ein äquivalentes Netz mit nur einem Faltungslayer mit linearer Aktivierungsfunktion, $f^{(L)}$ Output-Feature-Maps sowie mit den zugehörigen $f^{(L)}$ Kernel der Form $w\in\R^{m_1\times\cdots\times m_d \times f^{(0)}}$ mit $m_i=\sum_{l=1}^L m^{(l)}_i - (L-1)$.
\end{theorem}
\begin{proof}
    Wir nutzen für das gegebene $L$-Layer-Netz die Notation aus Definition~\ref{def:multi-dimensional-conv-layer}.
    Wie in Theorem~\ref{thm:matrix-collapse-linear-conv-layers} genügt es, die Aussage nur für den Fall $L=2$ zu zeigen; der Rest folgt auch hier induktiv.

    \newcommand{\bcdot}{\boldsymbol{\cdot}}
    Dazu benötigen wir folgende Notationen.
    Definieren wir \[
        w^{(l), k}_{\bcdot, k'}\coloneqq \left( w_{j,k'}^{(l),k}\right)_{j\in J^{(l)}}
        \quad \text{und} \quad
        x^{(l)}_{\bcdot, k} \coloneqq \left( x^{(l)}_{i,k} \right)_{i\in I^{(l)}}
        \quad \text{für $k\in \fNat{f^{(l)}}, k'\in\fNat{f^{(l-1)}}$},
    \]
    so können wir den Output eines Layers $l$ schreiben als 
    \begin{align*}
        x^{(l)}_{i, k}
        &=  \left( w^{(l),k} \star x^{(l-1)}\right)_{i,0} + b^{(l),k}
        %=  \left( \sum_{\substack{
        %    j\in J^{(l)}\\
        %    k'\in \fNat{f^{(l-1)}}
        %}}  w^{(l),k}_{j,k'} \cdot x^{(l-1)}_{i+j,k'} + b^{(l),k} \right)_{i\in I^{(l)}} \\
        = \sum_{k'\in \fNat{f^{(l-1)}}}  \left(w^{(l),k}_{\bcdot,k'} \star x^{(l-1)}_{\bcdot,k'} \right)_i + b^{(l),k}
    \end{align*}
    für $i\in I^{(l)}, k\in\fNat{f^{(l)}}$.
    Für den Output von Layer $2$ gilt dann für $i_2\in I^{(2)}, k_2\in \fNat{f^{(2)}}$
    \begin{align*}
        x^{(2)}_{i_2,k_2}
        &= \sum_{k_1\in \fNat{f^{(1)}}}  \left(w^{(2),k_2}_{\bcdot,k_1} \star x^{(1)}_{\bcdot,k_1} \right)_{i_2} + b^{(2),k_2} \\
        &= \sum_{k_1\in \fNat{f^{(1)}}}  \left(w^{(2),k_2}_{\bcdot,k_1} \star \left( 
%
            \sum_{k_0\in \fNat{f^{(0)}}}  \left(w^{(1),k_1}_{\bcdot,k_0} \star x^{(0)}_{\bcdot,k_0} \right)_{i_1} + b^{(1),k_1}
%
         \right)_{i_1\in I^{(1)}} \right)_{i_2} + b^{(2),k_2}.
    \end{align*}
    Da Kreuzkorrelation distributiv über Addition ist, lässt sich dieser Term mit \[
        b^{k_2}\coloneqq \sum_{k_1\in\fNat{f^{(1)}}}\sum_{j\in J^{(2)}} w^{(2), k_2}_{j,k_1} \cdot b^{(1),k_1} + b^{(2), k_2}
    \]
    \newcommand{\rest}[2]{\left.{#1}\right|_{#2}}
    und der Schreibweise $\rest{g}{K} \coloneqq (g_i)_{i\in K}$ für $g: \Z^d \rightarrow \R, K\subseteq \Z^d$ vereinfachen zu 
    \[
        x^{(2)}_{i_2,k_2} 
        = \sum_{k_0\in \fNat{f^{(0)}}}\sum_{k_1\in \fNat{f^{(1)}}}  \left(w^{(2),k_2}_{\bcdot,k_1} \star 
        \rest{\left(w^{(1),k_1}_{\bcdot,k_0} \star x^{(0)}_{\bcdot,k_0} \right)}{I^{(1)}}
        \right)_{i_2} + b^{k_2}.
    \]
    Aufgrund der Wahl von $I^{(2)}\ni i_2$ spielt die Einschränkung auf $I^{(1)}$ keine Rolle.
    Proposition~\ref{prop:associativity-cross-corr} impliziert nun
    \begin{align*}
        x^{(2)}_{i_2,k_2} 
        &= \sum_{k_0\in \fNat{f^{(0)}}}\sum_{k_1\in \fNat{f^{(1)}}}  \left( \left( w^{(2),k_2}_{\bcdot,k_1} * 
        w^{(1),k_1}_{\bcdot,k_0}\right) \star x^{(0)}_{\bcdot,k_0}
        \right)_{i_2} + b^{k_2} \\
        &= \sum_{k_0\in \fNat{f^{(0)}}} \left( \left(
            \sum_{k_1\in \fNat{f^{(1)}}}  w^{(2),k_2}_{\bcdot,k_1} * 
        w^{(1),k_1}_{\bcdot,k_0}\right) \star x^{(0)}_{\bcdot,k_0}
        \right)_{i_2} + b^{k_2}.
    \end{align*}
    Definieren wir den Kern $w^{k_2}_{\bcdot, k_0} \coloneqq \sum_{k_1\in \fNat{f^{(1)}}}  w^{(2),k_2}_{\bcdot,k_1} * 
    w^{(1),k_1}_{\bcdot,k_0}$, dessen Träger in der Menge $\bigtimes_{i=1}^d \fNat{m^{(1)}_i + m^{(2)}_i - 1}$ enthalten ist, erhalten wir
    \[
        x^{(2)}_{i_2, k_2}
        = \sum_{k_0\in \fNat{f^{(0)}}} \left( w^{k_2}_{\bcdot, k_0}\star x^{(0)}_{\bcdot,k_0}
        \right)_{i_2} + b^{k_2}
        = \left( w^{k_2} \star x^{(0)} \right)_{i_2, 0} + b^{k_2},
    \]
    sodass die Kerne $(w^{k_2})_{k_2\in\fNat{f^{(2)}}}$ zusammen mit den Biases $(b^{k_2})_{k_2\in\fNat{f^{(2)}}}$ ein äquivalentes Faltungslayer darstellen.
\end{proof}

Wir resümieren, dass es im Eindimensionalen zwar immer von Vorteil ist, aufeinanderfolgende Faltungslayer mit linearer Aktivierungsfunktion zu fusionieren, im Mehrdimensionalen ist dies zwar auch möglich, jedoch womöglich auf Kosten einer deutlich größeren Parameterzahl:

Hat man $L$ zweidimensionale Faltungslayer mit Kernel von $2\times 2$ räumlicher Größe, so ist die Anzahl an Parametern genau $5L$.
Fusioniert man diese jedoch zu einem einzigen Faltungslayer, so erhält man einen Kern mit räumlicher Größe $(L+1)^2$ und einer Parameterzahl von $L^2 + 2L + 2$.

In der Praxis werden Faltungslayer meist mit einer nicht-linearen Aktivierungsfunktion ausgestattet.

\subsection{Varianten der Faltungslayer}



\subsection{Äquivarianz}

\todo{
    
Eine wichtige Eigenschaft der den Faltungslayern zugrundeliegenden Kreuzkorrelation ist die sogenannte Äquivarianz bezüglich Translation.
In anderen Worten: Es ist egal, ob zuerst die Kreuzkorrelation bezüglich eines Filters $f$ angewandt wird und anschließend das Ergebnis verschoben wird oder ob zuerst das Eingangssignal verschoben wird und anschließend die Kreuzkorrelation bezüglich $f$ berechnet wird.

\begin{proposition}[Äquivarianz bzgl. Translation]
    Sei $f$ ein $L^1$-endliches Signal.
    Definiere $\mathcal{C}(g)\coloneqq f\star g$ als die Kreuzkorrelation mit $f$ und $\transl{a}{(g)}(x) \coloneqq g(x - a)$ für ein Signal $g$ und $a\in\R^d$.
    Dann gilt \[ \mathcal{C}(\transl{a}{(g)}) = \transl{a}{(\mathcal{C}(g))}. \]
\end{proposition}

\begin{proof}
    $\mathcal{C}(T_a\, g)(x) = \int_{\R^d} f(\tau) \cdot g(x - a + \tau) \diff \tau = \transl{a}{\mathcal{C}(g)}$
\end{proof}

}


\section{Pooling}



\section{Faltungsnetze}


\clearpage          % neue Seite für Literaturverzeichnis
\thispagestyle{empty}
\bibliography{literature}

\end{document}